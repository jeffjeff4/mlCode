####python raw implementation, implement raw python code, simple MLP çš„training loopï¼Œwith data parallel, tensor parallel.
####please generate data X, Y, and code to use the above DP and tensor parallel code
####
####è¯·ï¼š
####1. æŠŠè¿™ä¸ªä»£ç  æ”¹æˆæ”¯æŒ Adam ä¼˜åŒ–å™¨ã€
####2. TP å°½é‡ä¸ŽçœŸå®žçš„åˆ‡åˆ†æ–¹å¼ä¸€è‡´ï¼Œ çœŸå®žçŽ¯å¢ƒä¸­æ˜¯W1æŒ‰åˆ—åˆ‡ï¼ŒW2æŒ‰è¡Œåˆ‡
####ç¬¬ä¸€å±‚æƒé‡çŸ©é˜µ W1 æŒ‰åˆ—åˆ‡åˆ†ï¼ˆcolumn-wise partitioningï¼‰ã€‚
####ç¬¬äºŒå±‚æƒé‡çŸ©é˜µ W2 æŒ‰è¡Œåˆ‡åˆ†ï¼ˆrow-wise partitioningï¼‰ã€‚
####3. è¿™é‡Œç”¨ multiprocessing.Process + Queue è¿›è¡Œè¿›ç¨‹é—´é€šä¿¡ï¼ˆé€‚åˆæ•™å­¦ï¼‰ã€‚
####4. å¯ä»¥ä¸ä½¿ç”¨çœŸå®žæ¡†æž¶ä¸­ä¼šç”¨æ›´é«˜æ•ˆçš„é€šä¿¡ï¼ˆNCCL / AllGather / AllReduceï¼‰ã€‚
####   å¯ä»¥ä¸ä½¿ç”¨çœŸå®žéƒ¨ç½²åœ¨å¤š GPU ä¸Šä¼šä½¿ç”¨ GPU é€šä¿¡åº“ã€å…±äº«å†…å­˜ã€æ¢¯åº¦åŽ‹ç¼©ç­‰æŠ€å·§ã€‚
####5. è¯·ä¸è¦ä½¿ç”¨queueï¼Œä½¿ç”¨ç®€å•çš„æ–¹æ³•æ¥æ¨¡æ‹Ÿgpuä¹‹é—´çš„æ•°æ®äº¤äº’
####6. è¯·æ¨¡æ‹Ÿä½¿ç”¨2å—GPU
####7. è¯·æŠŠè¿™ä¸ª å¯è§†åŒ–å›¾è§£ï¼ˆç”¨ ascii blockï¼‰é›†æˆåˆ°ä¹‹å‰å†™çš„æ•™å­¦ä»£ç é‡Œï¼Œè¿™æ ·è·‘çš„æ—¶å€™å°±èƒ½çœ‹åˆ°çŸ©é˜µæ€Žä¹ˆåˆ‡åˆ†ã€æ•°æ®æ€Žä¹ˆæµåŠ¨ã€‚è¯·åŒ…æ‹¬forward propagation, backward propagation
####8. è¯·äº§ç”Ÿç±»ä¼¼å¦‚ä¸‹çš„æµç¨‹å›¾ï¼š
####=== Tensor Parallel Training with Adam ===
####
####TP Visualization - W1 Split (Column-Wise):
####
####  X (64 x 784)
####    |
####    v
####+---------+
####| W1      |
####| 784 x 256|
####+---------+
####Split column-wise:
####+---------+---------+
####| Device 0| Device 1|
####| 784 x 128| 784 x 128|
####+---------+---------+
####    |         |
####    v         v
#### A1_part0  A1_part1 (64 x 128)
#### (64 x 128)
#### (No All-Gather for A1; used locally for Z2_local)
####
####
####TP Visualization - W2 Split (Row-Wise):
####
#### A1_part (64 x 128)
####    |
####    v
####+---------+
####| W2      |
####| 256 x 10|
####+---------+
####Split row-wise:
####+---------+
####| Device 0|
####| 128 x 10|
####+---------+
####+---------+
####| Device 1|
####| 128 x 10|
####+---------+
####    |
####    v
#### Z2_local0 (64 x 10)
####    |
####    v
#### Z2_local1 (64 x 10)
####    |
####    +--------+
####      All-Reduce (sum)
####        |
####        v
#### Z2 (64 x 10)
####
####
####TP Visualization - Backward Flow:
####
#### dZ2 (64 x 10)
####    |
####    v
####dA1_local = dZ2 @ W2_part.T (64 x 128)
####    |
####    +--------+
####      All-Gather (concat)
####        |
####        v
#### dA1 (64 x 256)
####Split to dA1_slice (64 x 128)
####    |
####    v
#### dZ1_part = dA1_slice * relu_deriv (64 x 128)
####    |
####    v
#### dW1_part = X.T @ dZ1_part / m (784 x 128)
#### dW2_part = A1_part.T @ dZ2 / m (128 x 10)


####-------------------------------------------------------------------------------------
####çœŸå®žçŽ¯å¢ƒä¸­ä¸æ˜¯W1æŒ‰åˆ—åˆ‡ï¼ŒW2æŒ‰è¡Œåˆ‡å—ï¼Ÿä¸Šé¢è¿™ä¸ªcodeæ˜¯æŒ‰ç…§W1æŒ‰åˆ—åˆ‡ï¼ŒW2æŒ‰è¡Œåˆ‡çš„æ–¹å¼æ¥çš„å—ï¼Ÿ
####
####
####å¯¹ï¼Œä½ çš„ç†è§£æ˜¯å¯¹çš„ âœ…ã€‚â€¨åœ¨ çœŸå®žçš„ Tensor Parallel (TP) å®žçŽ°é‡Œï¼ˆæ¯”å¦‚ Megatron-LMï¼‰ï¼Œå¸¸è§çš„åˆ‡åˆ†æ–¹å¼æ˜¯ï¼š
####W1ï¼ˆè¾“å…¥å±‚ â†’ éšå±‚ï¼‰ï¼š
####W1 çš„ åˆ—åˆ‡åˆ†ï¼ˆcolumn parallelismï¼‰
####æ¯ä¸ª rank è´Ÿè´£ä¸€éƒ¨åˆ† è¾“å‡ºç»´åº¦ã€‚
####ä¼˜ç‚¹ï¼šè¾“å…¥ X å¯ä»¥å¹¿æ’­åˆ°æ‰€æœ‰ rankï¼Œå„è‡ªç®— X @ W1_shardï¼Œå¾—åˆ°çš„æ˜¯å¯¹åº” shard çš„éƒ¨åˆ† hiddenã€‚æœ€åŽå†æ‹¼èµ·æ¥ã€‚
####W2ï¼ˆéšå±‚ â†’ è¾“å‡ºå±‚ï¼‰ï¼š
####W2 çš„ è¡Œåˆ‡åˆ†ï¼ˆrow parallelismï¼‰
####æ¯ä¸ª rank è´Ÿè´£ä¸€éƒ¨åˆ† è¾“å…¥ç»´åº¦ã€‚
####ä¼˜ç‚¹ï¼šæ¯ä¸ª rank åªå­˜å‚¨éƒ¨åˆ† hidden çš„å‚æ•°ï¼Œforward æ—¶å„è‡ªå¤„ç† shardï¼Œå¯¹è¾“å‡ºåš AllReduceï¼ˆå› ä¸ºæœ€ç»ˆè¾“å‡ºè¦æ•´åˆæ‰€æœ‰ shard çš„è´¡çŒ®ï¼‰ã€‚
####
####ðŸ”Ž ä½ æåˆ°çš„æˆ‘çš„ç¤ºä¾‹ä»£ç 
####æˆ‘ä¹‹å‰ç»™ä½ çš„æ•™å­¦ç‰ˆä»£ç ï¼Œç¡®å®žæ˜¯è¿™æ ·åˆ‡çš„ï¼š
####W1 æ˜¯ torch.randn(D_in, D_hidden)ï¼Œåœ¨æž„é€ æ—¶æˆ‘ç”¨äº† åˆ—åˆ‡ â†’ torch.chunk(W1, tp_size, dim=1)ã€‚â€¨âœ” è¿™å°±æ˜¯ â€œW1 æŒ‰åˆ—åˆ‡â€ã€‚
####W2 æ˜¯ torch.randn(D_hidden, D_out)ï¼Œåœ¨æž„é€ æ—¶æˆ‘ç”¨äº† è¡Œåˆ‡ â†’ torch.chunk(W2, tp_size, dim=0)ã€‚â€¨âœ” è¿™å°±æ˜¯ â€œW2 æŒ‰è¡Œåˆ‡â€ã€‚
####æ‰€ä»¥å’Œä½ è¯´çš„ çœŸå®žçŽ¯å¢ƒå¸¸è§åšæ³•æ˜¯ä¸€è‡´çš„ã€‚
####
####è¦ä¸è¦æˆ‘å¸®ä½ åœ¨ä»£ç é‡ŒåŠ ä¸€æ®µ å›¾ç¤ºï¼ˆçŸ©é˜µåˆ‡åˆ†å¯è§†åŒ–ï¼‰ï¼Œè¿™æ ·å¯ä»¥æ¸…æ¥šçœ‹åˆ°ï¼š
####W1 å¦‚ä½•æŒ‰åˆ—åˆ‡åˆ†
####W2 å¦‚ä½•æŒ‰è¡Œåˆ‡åˆ†â€¨ä»¥åŠ forward/backward ä¸­å„ä¸ª shard çš„æ•°æ®æµï¼Ÿ
####
####
####
####å¥½ ðŸ‘Œ æˆ‘æ¥å¸®ä½ æŠŠ Tensor Parallel çš„çŸ©é˜µåˆ‡åˆ†æ–¹å¼ ç”¨å›¾ç¤ºç›´è§‚å±•ç¤ºä¸€ä¸‹ã€‚
####
####ðŸ”¹ 1. W1 æŒ‰åˆ—åˆ‡åˆ†ï¼ˆColumn Parallelï¼‰
####è®¾è¾“å…¥å±‚ç»´åº¦æ˜¯ D_inï¼Œéšå±‚ç»´åº¦æ˜¯ D_hiddenã€‚â€¨æƒé‡çŸ©é˜µ W1 çš„å½¢çŠ¶æ˜¯ (D_in, D_hidden)ã€‚
####æ­£å¸¸æƒ…å†µï¼š
####
####X (N, D_in)  @  W1 (D_in, D_hidden)  â†’  H (N, D_hidden)
####åˆ—åˆ‡åˆ†ï¼š
####
####W1 è¢«æŒ‰åˆ—åˆ‡åˆ† â†’ æ¯ä¸ª rank æ‹¿ä¸€éƒ¨åˆ† hidden
####ä¾‹å­ï¼šâ€¨å‡è®¾ D_in = 4, D_hidden = 6, åˆ‡æˆ 2 ä»½ï¼š
####
####         W1 (4x6)
####   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
####   â”‚      Rank0    â”‚      Rank1
####   â”‚   (4x3 cols)  â”‚   (4x3 cols)
####   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
####æ¯ä¸ª rank ç®—ï¼š
####
####H0 = X @ W1_shard0   â†’ (N, 3)
####H1 = X @ W1_shard1   â†’ (N, 3)
####æœ€åŽæ‹¼æŽ¥ï¼š
####
####H = [H0 | H1]  â†’ (N, 6)
####
####ðŸ”¹ 2. W2 æŒ‰è¡Œåˆ‡åˆ†ï¼ˆRow Parallelï¼‰
####è®¾éšå±‚ç»´åº¦æ˜¯ D_hiddenï¼Œè¾“å‡ºå±‚ç»´åº¦æ˜¯ D_outã€‚â€¨æƒé‡çŸ©é˜µ W2 çš„å½¢çŠ¶æ˜¯ (D_hidden, D_out)ã€‚
####æ­£å¸¸æƒ…å†µï¼š
####
####H (N, D_hidden)  @  W2 (D_hidden, D_out)  â†’  Y (N, D_out)
####è¡Œåˆ‡åˆ†ï¼š
####
####W2 è¢«æŒ‰è¡Œåˆ‡åˆ† â†’ æ¯ä¸ª rank æ‹¿ä¸€éƒ¨åˆ†è¾“å…¥ hidden
####ä¾‹å­ï¼šâ€¨å‡è®¾ D_hidden = 6, D_out = 2, åˆ‡æˆ 2 ä»½ï¼š
####
####         W2 (6x2)
####   â”Œâ”€â”€â”€â”€â”€â”€â”€â”
####   â”‚ Rank0 â”‚  (3x2 rows)
####   â”‚ Rank1 â”‚  (3x2 rows)
####   â””â”€â”€â”€â”€â”€â”€â”€â”˜
####æ¯ä¸ª rank ç®—ï¼š
####
####Y0 = H[:, :3] @ W2_shard0   â†’ (N, 2)
####Y1 = H[:, 3:] @ W2_shard1   â†’ (N, 2)
####æœ€åŽ åŠ å’Œ (AllReduce)ï¼š
####
####Y = Y0 + Y1   â†’ (N, 2)
####
####ðŸ”¹ 3. æ€»ç»“æ•°æ®æµ
####
####X â”€â”€â–º W1 (col split) â”€â”€â–º æ‹¼æŽ¥ H â”€â”€â–º W2 (row split) â”€â”€â–º AllReduce â”€â”€â–º Y
####W1ï¼šåˆ—åˆ‡ â†’ è¾“å‡ºæ‹¼æŽ¥ï¼ˆConcatï¼‰
####W2ï¼šè¡Œåˆ‡ â†’ è¾“å‡ºæ±‚å’Œï¼ˆReduceï¼‰
####
####è¦ä¸è¦æˆ‘ç›´æŽ¥æŠŠè¿™ä¸ª å¯è§†åŒ–å›¾è§£ï¼ˆç”¨ matplotlib æˆ– ascii blockï¼‰é›†æˆåˆ°æˆ‘ä¹‹å‰å†™çš„æ•™å­¦ä»£ç é‡Œï¼Œè¿™æ ·ä½ è·‘çš„æ—¶å€™å°±èƒ½çœ‹åˆ°çŸ©é˜µæ€Žä¹ˆåˆ‡åˆ†ã€æ•°æ®æ€Žä¹ˆæµåŠ¨ï¼Ÿ
####
####
####
####è¯·åŠ åå‘ä¼ æ’­çš„ ascii blockï¼Œæ˜¾ç¤º grad_W1 å’Œ grad_W2 æ˜¯æ€Žä¹ˆåˆå¹¶çš„
####
####
####
####å¥½ âœ…â€¨åœ¨å‰å‘æˆ‘ä»¬å·²ç»ç”»äº† W1 åˆ—åˆ‡ / W2 è¡Œåˆ‡ï¼ŒçŽ°åœ¨æˆ‘æ¥å¸®ä½ è¡¥ä¸Š åå‘ä¼ æ’­çš„ ascii blockï¼Œå¹¶ä¸”æŠŠå®ƒé›†æˆåˆ°ä»£ç é‡Œã€‚è¿™æ ·ä¸€è·‘ï¼Œä¸å…‰èƒ½çœ‹åˆ° forward çš„æ•°æ®æµï¼Œè¿˜èƒ½çœ‹åˆ° backward çš„æ¢¯åº¦æµã€‚
####
####ðŸ”¹ åå‘ä¼ æ’­ç›´è§‚é€»è¾‘
####Loss å¯¹è¾“å‡ºçš„æ¢¯åº¦
####
####dL/dY  (N, D_out)
####W2ï¼ˆè¡Œåˆ‡åˆ†ï¼‰çš„åä¼ 
####forward æ—¶ W2 æ˜¯ è¡Œåˆ‡ â†’ æ¯ä¸ª rank è´Ÿè´£ä¸€éƒ¨åˆ† hidden â†’ åä¼ æ—¶æ¯ä¸ª rank åªæ”¶åˆ°å¯¹åº” shard çš„ grad_H_localã€‚
####
####grad_W2_rank = H_localáµ€ @ dL/dY
####grad_H_local = dL/dY @ W2_shardáµ€
####æœ€åŽï¼š
####
####grad_W2 = sum(grad_W2_rank)   (è¡Œæ‹¼èµ·æ¥)
####grad_H  = concat(grad_H_local)  (åˆ—æ‹¼èµ·æ¥)
####W1ï¼ˆåˆ—åˆ‡åˆ†ï¼‰çš„åä¼ 
####forward æ—¶ W1 æ˜¯ åˆ—åˆ‡ â†’ æ¯ä¸ª rank åªè®¡ç®—éƒ¨åˆ† H â†’ backward æ—¶è¦ sum å„ä¸ª shard å¯¹è¾“å…¥çš„æ¢¯åº¦è´¡çŒ®ï¼š
####
####grad_W1_rank = Xáµ€ @ grad_H_local
####grad_X       = sum_over_ranks(grad_H_local @ W1_shardáµ€)
####æœ€åŽï¼š
####
####grad_W1 = sum(grad_W1_rank)



import numpy as np
import multiprocessing as mp
from multiprocessing import Process, Barrier, Lock, Array

def create_shared_array(shape):
    size = int(np.prod(shape))
    arr = Array('d', size)
    return np.frombuffer(arr.get_obj(), dtype=np.float64).reshape(shape)

def relu(x):
    return np.maximum(0, x)

def relu_deriv(x):
    return (x > 0).astype(np.float64)

def mse_loss(pred, target):
    return np.mean((pred - target)**2)

def mse_grad(pred, target):
    return 2 * (pred - target) / pred.shape[0]

def adam_update(param, grad, m, v, lr, beta1, beta2, eps, t):
    m = beta1 * m + (1 - beta1) * grad
    v = beta2 * v + (1 - beta2) * np.square(grad)
    m_hat = m / (1 - beta1**t)
    v_hat = v / (1 - beta2**t)
    param -= lr * m_hat / (np.sqrt(v_hat) + eps)
    return param, m, v

def print_ascii_matrix(rank, title, matrix, max_cols=10, max_rows=5):
    # Limit print size for large matrices, but since small dims, ok
    print(f"Rank {rank}: {title}")
    rows, cols = matrix.shape
    if rows > max_rows:
        print(f"(Showing first {max_rows} rows of {rows})")
        matrix = matrix[:max_rows]
        rows = max_rows
    if cols > max_cols:
        print(f"(Showing first {max_cols} cols of {cols})")
        matrix = matrix[:, :max_cols]
        cols = max_cols
    print("+" + "-" * (cols * 6 + cols - 1) + "+")
    for row in matrix:
        s = "| " + " | ".join(f"{x: .2f}" for x in row) + " |"
        print(s)
    print("+" + "-" * (cols * 6 + cols - 1) + "+")

def print_flow(rank, message):
    print(f"Rank {rank}: {message}")

def print_tp_visualization(batch_size, input_dim, hidden_dim, output_dim, TP_size):
    local_hidden = hidden_dim // TP_size
    print("=== Tensor Parallel Training with Adam ===")

    print("\nTP Visualization - W1 Split (Column-Wise):\n")

    print(f"  X ({batch_size} x {input_dim})")
    print("    |")
    print("    v")
    print("+-----------------+")
    print(f"| W1              |")
    print(f"| {input_dim} x {hidden_dim}   |")
    print("+-----------------+")
    print("Split column-wise:")
    print("+---------+---------+")
    print("| Device 0| Device 1|")
    print(f"| {input_dim} x {local_hidden}| {input_dim} x {local_hidden}|")
    print("+---------+---------+")
    print("    |         |")
    print("    v         v")
    print(f" h_part0   h_part1 ({batch_size} x {local_hidden})")
    print(f" ({batch_size} x {local_hidden})")
    print(" (No All-Gather for h; used locally for logit_local)")

    print("\nTP Visualization - W2 Split (Row-Wise):\n")

    print(f" activated_part ({batch_size} x {local_hidden})")
    print("    |")
    print("    v")
    print("+-----------------+")
    print(f"| W2              |")
    print(f"| {hidden_dim} x {output_dim}   |")
    print("+-----------------+")
    print("Split row-wise:")
    print("+---------+")
    print("| Device 0|")
    print(f"| {local_hidden} x {output_dim}|")
    print("+---------+")
    print("+---------+")
    print("| Device 1|")
    print(f"| {local_hidden} x {output_dim}|")
    print("+---------+")
    print("    |       ")
    print("    v       ")
    print(f" logit_local0 ({batch_size} x {output_dim})")
    print("    |")
    print("    v")
    print(f" logit_local1 ({batch_size} x {output_dim})")
    print("    |")
    print("    +--------+")
    print("      All-Reduce (sum)")
    print("        |")
    print("        v")
    print(f" logit ({batch_size} x {output_dim})")

    print("\nTP Visualization - Backward Flow:\n")

    print(f" d_logit ({batch_size} x {output_dim})")
    print("    |")
    print("    v")
    print(f"d_activated_local = d_logit @ W2_part.T ({batch_size} x {local_hidden})  # sharded directly")
    print("    |")
    print("    v")
    print(f" d_h_part = d_activated_local * relu_deriv(h_local) ({batch_size} x {local_hidden})")
    print("    |")
    print("    v")
    print(f" grad_W1_part = X.T @ d_h_part / {batch_size} ({input_dim} x {local_hidden})")
    print(f" grad_W2_part = activated_part.T @ d_logit / {batch_size} ({local_hidden} x {output_dim})")

def worker(rank, TP_size, DP_size, world_size, input_dim, hidden_dim, output_dim, N, micro_batch_size, epochs, lr, beta1, beta2, eps,
           shared_X, shared_Y, shared_W1_shards, shared_W2_shards, shared_b1_shards, shared_b2,
           shared_logit_buffers, shared_grad_W1_buffers, shared_grad_W2_buffers, shared_grad_b1_buffers, shared_grad_b2_buffers,
           barrier, lock):
    local_hidden = hidden_dim // TP_size
    tp_rank = rank % TP_size
    dp_rank = rank // TP_size
    local_N = N // DP_size
    start_idx = dp_rank * local_N
    end_idx = start_idx + local_N

    # Copy local data
    X_local = np.copy(shared_X[start_idx:end_idx])
    Y_local = np.copy(shared_Y[start_idx:end_idx])

    # Copy local model shards
    W1_local = np.copy(shared_W1_shards[tp_rank])
    W2_local = np.copy(shared_W2_shards[tp_rank])
    b1_local = np.copy(shared_b1_shards[tp_rank])
    b2_local = np.copy(shared_b2)

    # Visualize initial shards
    print_ascii_matrix(rank, f"Initial W1 shard (column-wise split)", W1_local)
    print_ascii_matrix(rank, f"Initial W2 shard (row-wise split)", W2_local)
    print_ascii_matrix(rank, f"Initial b1 shard", b1_local.reshape(1, -1))
    print_ascii_matrix(rank, f"Initial b2 (replicated)", b2_local.reshape(1, -1))

    # Adam states
    m_W1 = np.zeros_like(W1_local)
    v_W1 = np.zeros_like(W1_local)
    m_W2 = np.zeros_like(W2_local)
    v_W2 = np.zeros_like(W2_local)
    m_b1 = np.zeros_like(b1_local)
    v_b1 = np.zeros_like(b1_local)
    m_b2 = np.zeros_like(b2_local)
    v_b2 = np.zeros_like(b2_local)

    num_batches = local_N // micro_batch_size

    for epoch in range(epochs):
        for batch_idx in range(num_batches):
            batch_start = batch_idx * micro_batch_size
            batch_end = batch_start + micro_batch_size
            X_batch = X_local[batch_start:batch_end]
            Y_batch = Y_local[batch_start:batch_end]

            # Forward pass
            print_flow(rank, f"Forward: local_h = X_batch @ W1_local + b1_local | Shapes: {X_batch.shape} @ {W1_local.shape} -> {X_batch.shape[0], local_hidden}")
            local_h = X_batch @ W1_local + b1_local
            print_ascii_matrix(rank, "local_h (sharded hidden)", local_h)

            print_flow(rank, "Forward: activated = relu(local_h)")
            activated = relu(local_h)
            print_ascii_matrix(rank, "activated (sharded)", activated)

            print_flow(rank, f"Forward: local_logit = activated @ W2_local + b2_local | Shapes: {activated.shape} @ {W2_local.shape} -> {activated.shape[0], output_dim}")
            local_logit = activated @ W2_local + b2_local
            print_ascii_matrix(rank, "local_logit (partial sum)", local_logit)

            # All-reduce sum for logit across TP (simulate with shared memory)
            shared_logit = shared_logit_buffers[dp_rank]
            with lock:
                if tp_rank == 0:
                    shared_logit.fill(0.0)
            barrier.wait()
            with lock:
                shared_logit += local_logit
            barrier.wait()
            logit = np.copy(shared_logit)
            print_flow(rank, "Forward: After all-reduce sum across TP -> full logit")
            print_ascii_matrix(rank, "full logit", logit)

            # Loss (for logging)
            loss = mse_loss(logit, Y_batch)
            print_flow(rank, f"Loss: {loss}")

            # Backward pass
            d_logit = mse_grad(logit, Y_batch)
            print_flow(rank, "Backward: d_logit (full)")
            print_ascii_matrix(rank, "d_logit", d_logit)

            # Grad for b2 (replicated)
            grad_b2 = np.sum(d_logit, axis=0)

            # Grad for W2
            print_flow(rank, "Backward: grad_W2 = activated.T @ d_logit")
            grad_W2 = activated.T @ d_logit

            # d_activated (sharded)
            print_flow(rank, "Backward: d_activated = d_logit @ W2_local.T")
            d_activated = d_logit @ W2_local.T
            print_ascii_matrix(rank, "d_activated (sharded)", d_activated)

            # Activation deriv
            d_local_h = d_activated * relu_deriv(local_h)

            # Grad for b1 (sharded)
            grad_b1 = np.sum(d_local_h, axis=0)

            # Grad for W1
            print_flow(rank, "Backward: grad_W1 = X_batch.T @ d_local_h")
            grad_W1 = X_batch.T @ d_local_h

            # All-reduce grads across DP (sum then avg)
            # For W1 grad
            shared_grad_W1 = shared_grad_W1_buffers[tp_rank]
            with lock:
                if dp_rank == 0:
                    shared_grad_W1.fill(0.0)
            barrier.wait()
            with lock:
                shared_grad_W1 += grad_W1
            barrier.wait()
            avg_grad_W1 = shared_grad_W1 / DP_size

            # For W2
            shared_grad_W2 = shared_grad_W2_buffers[tp_rank]
            with lock:
                if dp_rank == 0:
                    shared_grad_W2.fill(0.0)
            barrier.wait()
            with lock:
                shared_grad_W2 += grad_W2
            barrier.wait()
            avg_grad_W2 = shared_grad_W2 / DP_size

            # For b1
            shared_grad_b1 = shared_grad_b1_buffers[tp_rank]
            with lock:
                if dp_rank == 0:
                    shared_grad_b1.fill(0.0)
            barrier.wait()
            with lock:
                shared_grad_b1 += grad_b1
            barrier.wait()
            avg_grad_b1 = shared_grad_b1 / DP_size

            # For b2 (replicated, avg across DP, add only once per DP)
            shared_grad_b2 = shared_grad_b2_buffers[0]
            with lock:
                if dp_rank == 0 and tp_rank == 0:
                    shared_grad_b2.fill(0.0)
            barrier.wait()
            if tp_rank == 0:
                with lock:
                    shared_grad_b2 += grad_b2
            barrier.wait()
            avg_grad_b2 = shared_grad_b2 / DP_size

            # Adam update
            t = epoch * num_batches + batch_idx + 1
            W1_local, m_W1, v_W1 = adam_update(W1_local, avg_grad_W1, m_W1, v_W1, lr, beta1, beta2, eps, t)
            W2_local, m_W2, v_W2 = adam_update(W2_local, avg_grad_W2, m_W2, v_W2, lr, beta1, beta2, eps, t)
            b1_local, m_b1, v_b1 = adam_update(b1_local, avg_grad_b1, m_b1, v_b1, lr, beta1, beta2, eps, t)
            b2_local, m_b2, v_b2 = adam_update(b2_local, avg_grad_b2, m_b2, v_b2, lr, beta1, beta2, eps, t)

            # Visualize updated shards (less frequent)
            if batch_idx % 2 == 0:
                print_ascii_matrix(rank, f"Updated W1 shard after batch {batch_idx}", W1_local)
                print_ascii_matrix(rank, f"Updated W2 shard after batch {batch_idx}", W2_local)

    print(f"Rank {rank}: Training complete.")

if __name__ == "__main__":
    TP_size = 2
    DP_size = 1  # Set to 1 for simulating 2 GPUs with TP=2
    world_size = TP_size * DP_size
    input_dim = 4
    hidden_dim = 16  # Divisible by TP_size
    output_dim = 2
    local_hidden = hidden_dim // TP_size
    N = 4  # Total samples
    micro_batch_size = 2
    epochs = 3
    lr = 0.01
    beta1 = 0.9
    beta2 = 0.999
    eps = 1e-8

    # Print visualization flowchart
    print_tp_visualization(micro_batch_size, input_dim, hidden_dim, output_dim, TP_size)

    # Generate data
    np.random.seed(42)
    shared_X = create_shared_array((N, input_dim))
    shared_Y = create_shared_array((N, output_dim))
    shared_X[:] = np.random.randn(N, input_dim)
    shared_Y[:] = np.random.randn(N, output_dim)

    # Initialize model shards
    shared_W1_shards = [create_shared_array((input_dim, local_hidden)) for _ in range(TP_size)]
    shared_W2_shards = [create_shared_array((local_hidden, output_dim)) for _ in range(TP_size)]
    shared_b1_shards = [create_shared_array((local_hidden,)) for _ in range(TP_size)]
    shared_b2 = create_shared_array((output_dim,))

    for i in range(TP_size):
        shared_W1_shards[i][:] = np.random.randn(input_dim, local_hidden) * 0.1
        shared_W2_shards[i][:] = np.random.randn(local_hidden, output_dim) * 0.1
        shared_b1_shards[i][:] = np.random.randn(local_hidden) * 0.1
    shared_b2[:] = np.random.randn(output_dim) * 0.1

    # Shared buffers
    shared_logit_buffers = [create_shared_array((micro_batch_size, output_dim)) for _ in range(DP_size)]
    shared_grad_W1_buffers = [create_shared_array((input_dim, local_hidden)) for _ in range(TP_size)]
    shared_grad_W2_buffers = [create_shared_array((local_hidden, output_dim)) for _ in range(TP_size)]
    shared_grad_b1_buffers = [create_shared_array((local_hidden,)) for _ in range(TP_size)]
    shared_grad_b2_buffers = [create_shared_array((output_dim,)) for _ in range(TP_size)]  # Use [0]

    barrier = Barrier(world_size)
    lock = Lock()

    processes = []
    for rank in range(world_size):
        p = Process(target=worker, args=(rank, TP_size, DP_size, world_size, input_dim, hidden_dim, output_dim, N, micro_batch_size, epochs, lr, beta1, beta2, eps,
                                         shared_X, shared_Y, shared_W1_shards, shared_W2_shards, shared_b1_shards, shared_b2,
                                         shared_logit_buffers, shared_grad_W1_buffers, shared_grad_W2_buffers, shared_grad_b1_buffers, shared_grad_b2_buffers,
                                         barrier, lock))
        p.start()
        processes.append(p)

    for p in processes:
        p.join()

    print("shared_W1_shards = ", shared_W1_shards)
    print("shared_grad_W1_buffers = ", shared_grad_W1_buffers)
    print("shared_W2_shards = ", shared_W2_shards)
    print("shared_grad_W2_buffers = ", shared_grad_W2_buffers)
    print("shared_b1_shards = ", shared_b1_shards)
    print("shared_grad_b1_buffers = ", shared_grad_b1_buffers)
    print("shared_b2 = ", shared_b2)
    print("shared_grad_b2_buffers = ", shared_grad_b2_buffers)



