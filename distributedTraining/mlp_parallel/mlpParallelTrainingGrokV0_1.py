####runnable, use for interview
####grok
####runnable code, using queue
####
####python raw implementation, implement raw python code, simple MLP çš„training loopï¼Œwith data parallel, tensor parallel.
####please generate data X, Y, and code to use the above DP and tensor parallel code
####
####è¯·ï¼š
####1. æŠŠè¿™ä¸ªä»£ç  æ”¹æˆæ”¯æŒ Adam ä¼˜åŒ–å™¨ã€
####2. TP å°½é‡ä¸ŽçœŸå®žçš„åˆ‡åˆ†æ–¹å¼ä¸€è‡´ï¼Œ çœŸå®žçŽ¯å¢ƒä¸­æ˜¯W1æŒ‰åˆ—åˆ‡ï¼ŒW2æŒ‰è¡Œåˆ‡
####ç¬¬ä¸€å±‚æƒé‡çŸ©é˜µ W1 æŒ‰åˆ—åˆ‡åˆ†ï¼ˆcolumn-wise partitioningï¼‰ã€‚
####ç¬¬äºŒå±‚æƒé‡çŸ©é˜µ W2 æŒ‰è¡Œåˆ‡åˆ†ï¼ˆrow-wise partitioningï¼‰ã€‚
####3. è¿™é‡Œç”¨ multiprocessing.Process + Queue è¿›è¡Œè¿›ç¨‹é—´é€šä¿¡ï¼ˆé€‚åˆæ•™å­¦ï¼‰ã€‚
####4. å¯ä»¥ä¸ä½¿ç”¨çœŸå®žæ¡†æž¶ä¸­ä¼šç”¨æ›´é«˜æ•ˆçš„é€šä¿¡ï¼ˆNCCL / AllGather / AllReduceï¼‰ã€‚
####   å¯ä»¥ä¸ä½¿ç”¨çœŸå®žéƒ¨ç½²åœ¨å¤š GPU ä¸Šä¼šä½¿ç”¨ GPU é€šä¿¡åº“ã€å…±äº«å†…å­˜ã€æ¢¯åº¦åŽ‹ç¼©ç­‰æŠ€å·§ã€‚
####6. è¯·æ¨¡æ‹Ÿä½¿ç”¨2å—GPU
####7. è¯·æŠŠè¿™ä¸ª å¯è§†åŒ–å›¾è§£ï¼ˆç”¨ ascii blockï¼‰é›†æˆåˆ°ä¹‹å‰å†™çš„æ•™å­¦ä»£ç é‡Œï¼Œè¿™æ ·è·‘çš„æ—¶å€™å°±èƒ½çœ‹åˆ°çŸ©é˜µæ€Žä¹ˆåˆ‡åˆ†ã€æ•°æ®æ€Žä¹ˆæµåŠ¨ã€‚è¯·åŒ…æ‹¬forward propagation, backward propagation
####-------------------------------------------------------------------------------------------------------------
####python raw implementation, implement raw python code, simple MLP çš„training loopï¼Œwith data parallel, tensor parallel.
####please generate data X, Y, and code to use the above DP and tensor parallel code
####
####è¯·ï¼š
####1. æŠŠè¿™ä¸ªä»£ç  æ”¹æˆæ”¯æŒ Adam ä¼˜åŒ–å™¨ã€
####2. TP å°½é‡ä¸ŽçœŸå®žçš„åˆ‡åˆ†æ–¹å¼ä¸€è‡´ï¼Œ çœŸå®žçŽ¯å¢ƒä¸­æ˜¯W1æŒ‰åˆ—åˆ‡ï¼ŒW2æŒ‰è¡Œåˆ‡
####ç¬¬ä¸€å±‚æƒé‡çŸ©é˜µ W1 æŒ‰åˆ—åˆ‡åˆ†ï¼ˆcolumn-wise partitioningï¼‰ã€‚
####ç¬¬äºŒå±‚æƒé‡çŸ©é˜µ W2 æŒ‰è¡Œåˆ‡åˆ†ï¼ˆrow-wise partitioningï¼‰ã€‚
####3. è¿™é‡Œç”¨ multiprocessing.Process + Queue è¿›è¡Œè¿›ç¨‹é—´é€šä¿¡ï¼ˆé€‚åˆæ•™å­¦ï¼‰ã€‚
####4. å¯ä»¥ä¸ä½¿ç”¨çœŸå®žæ¡†æž¶ä¸­ä¼šç”¨æ›´é«˜æ•ˆçš„é€šä¿¡ï¼ˆNCCL / AllGather / AllReduceï¼‰ã€‚
####   å¯ä»¥ä¸ä½¿ç”¨çœŸå®žéƒ¨ç½²åœ¨å¤š GPU ä¸Šä¼šä½¿ç”¨ GPU é€šä¿¡åº“ã€å…±äº«å†…å­˜ã€æ¢¯åº¦åŽ‹ç¼©ç­‰æŠ€å·§ã€‚
####5. è¯·ä¸è¦ä½¿ç”¨queueï¼Œä½¿ç”¨ç®€å•çš„æ–¹æ³•æ¥æ¨¡æ‹Ÿgpuä¹‹é—´çš„æ•°æ®äº¤äº’
####6. è¯·æ¨¡æ‹Ÿä½¿ç”¨2å—GPU
####7. è¯·æŠŠè¿™ä¸ª å¯è§†åŒ–å›¾è§£ï¼ˆç”¨ ascii blockï¼‰é›†æˆåˆ°ä¹‹å‰å†™çš„æ•™å­¦ä»£ç é‡Œï¼Œè¿™æ ·è·‘çš„æ—¶å€™å°±èƒ½çœ‹åˆ°çŸ©é˜µæ€Žä¹ˆåˆ‡åˆ†ã€æ•°æ®æ€Žä¹ˆæµåŠ¨ã€‚è¯·åŒ…æ‹¬forward propagation, backward propagation
####8. è¯·äº§ç”Ÿç±»ä¼¼å¦‚ä¸‹çš„æµç¨‹å›¾ï¼š
####=== Tensor Parallel Training with Adam ===
####
####TP Visualization - W1 Split (Column-Wise):
####
####  X (64 x 784)
####    |
####    v
####+---------+
####| W1      |
####| 784 x 256|
####+---------+
####Split column-wise:
####+---------+---------+
####| Device 0| Device 1|
####| 784 x 128| 784 x 128|
####+---------+---------+
####    |         |
####    v         v
#### A1_part0  A1_part1 (64 x 128)
#### (64 x 128)
#### (No All-Gather for A1; used locally for Z2_local)
####
####
####TP Visualization - W2 Split (Row-Wise):
####
#### A1_part (64 x 128)
####    |
####    v
####+---------+
####| W2      |
####| 256 x 10|
####+---------+
####Split row-wise:
####+---------+
####| Device 0|
####| 128 x 10|
####+---------+
####+---------+
####| Device 1|
####| 128 x 10|
####+---------+
####    |
####    v
#### Z2_local0 (64 x 10)
####    |
####    v
#### Z2_local1 (64 x 10)
####    |
####    +--------+
####      All-Reduce (sum)
####        |
####        v
#### Z2 (64 x 10)
####
####
####TP Visualization - Backward Flow:
####
#### dZ2 (64 x 10)
####    |
####    v
####dA1_local = dZ2 @ W2_part.T (64 x 128)
####    |
####    +--------+
####      All-Gather (concat)
####        |
####        v
#### dA1 (64 x 256)
####Split to dA1_slice (64 x 128)
####    |
####    v
#### dZ1_part = dA1_slice * relu_deriv (64 x 128)
####    |
####    v
#### dW1_part = X.T @ dZ1_part / m (784 x 128)
#### dW2_part = A1_part.T @ dZ2 / m (128 x 10)
####
####
####çœŸå®žçŽ¯å¢ƒä¸­ä¸æ˜¯W1æŒ‰åˆ—åˆ‡ï¼ŒW2æŒ‰è¡Œåˆ‡å—ï¼Ÿè¿™ä¸ªcodeæ˜¯æŒ‰ç…§W1æŒ‰åˆ—åˆ‡ï¼ŒW2æŒ‰è¡Œåˆ‡çš„æ–¹å¼æ¥çš„å—ï¼Ÿ
####
####torch.chunk(W1, tp_size, dim=1)
####
####è¯·æŠŠè¿™ä¸ª å¯è§†åŒ–å›¾è§£ï¼ˆç”¨ ascii blockï¼‰é›†æˆåˆ°ä¹‹å‰å†™çš„æ•™å­¦ä»£ç é‡Œï¼Œè¿™æ ·è·‘çš„æ—¶å€™å°±èƒ½çœ‹åˆ°çŸ©é˜µæ€Žä¹ˆåˆ‡åˆ†ã€æ•°æ®æ€Žä¹ˆæµåŠ¨ï¼Ÿ

####-------------------------------------------------------------------------------------
####çœŸå®žçŽ¯å¢ƒä¸­ä¸æ˜¯W1æŒ‰åˆ—åˆ‡ï¼ŒW2æŒ‰è¡Œåˆ‡å—ï¼Ÿä¸Šé¢è¿™ä¸ªcodeæ˜¯æŒ‰ç…§W1æŒ‰åˆ—åˆ‡ï¼ŒW2æŒ‰è¡Œåˆ‡çš„æ–¹å¼æ¥çš„å—ï¼Ÿ
####
####
####å¯¹ï¼Œä½ çš„ç†è§£æ˜¯å¯¹çš„ âœ…ã€‚â€¨åœ¨ çœŸå®žçš„ Tensor Parallel (TP) å®žçŽ°é‡Œï¼ˆæ¯”å¦‚ Megatron-LMï¼‰ï¼Œå¸¸è§çš„åˆ‡åˆ†æ–¹å¼æ˜¯ï¼š
####W1ï¼ˆè¾“å…¥å±‚ â†’ éšå±‚ï¼‰ï¼š
####W1 çš„ åˆ—åˆ‡åˆ†ï¼ˆcolumn parallelismï¼‰
####æ¯ä¸ª rank è´Ÿè´£ä¸€éƒ¨åˆ† è¾“å‡ºç»´åº¦ã€‚
####ä¼˜ç‚¹ï¼šè¾“å…¥ X å¯ä»¥å¹¿æ’­åˆ°æ‰€æœ‰ rankï¼Œå„è‡ªç®— X @ W1_shardï¼Œå¾—åˆ°çš„æ˜¯å¯¹åº” shard çš„éƒ¨åˆ† hiddenã€‚æœ€åŽå†æ‹¼èµ·æ¥ã€‚
####W2ï¼ˆéšå±‚ â†’ è¾“å‡ºå±‚ï¼‰ï¼š
####W2 çš„ è¡Œåˆ‡åˆ†ï¼ˆrow parallelismï¼‰
####æ¯ä¸ª rank è´Ÿè´£ä¸€éƒ¨åˆ† è¾“å…¥ç»´åº¦ã€‚
####ä¼˜ç‚¹ï¼šæ¯ä¸ª rank åªå­˜å‚¨éƒ¨åˆ† hidden çš„å‚æ•°ï¼Œforward æ—¶å„è‡ªå¤„ç† shardï¼Œå¯¹è¾“å‡ºåš AllReduceï¼ˆå› ä¸ºæœ€ç»ˆè¾“å‡ºè¦æ•´åˆæ‰€æœ‰ shard çš„è´¡çŒ®ï¼‰ã€‚
####
####ðŸ”Ž ä½ æåˆ°çš„æˆ‘çš„ç¤ºä¾‹ä»£ç 
####æˆ‘ä¹‹å‰ç»™ä½ çš„æ•™å­¦ç‰ˆä»£ç ï¼Œç¡®å®žæ˜¯è¿™æ ·åˆ‡çš„ï¼š
####W1 æ˜¯ torch.randn(D_in, D_hidden)ï¼Œåœ¨æž„é€ æ—¶æˆ‘ç”¨äº† åˆ—åˆ‡ â†’ torch.chunk(W1, tp_size, dim=1)ã€‚â€¨âœ” è¿™å°±æ˜¯ â€œW1 æŒ‰åˆ—åˆ‡â€ã€‚
####W2 æ˜¯ torch.randn(D_hidden, D_out)ï¼Œåœ¨æž„é€ æ—¶æˆ‘ç”¨äº† è¡Œåˆ‡ â†’ torch.chunk(W2, tp_size, dim=0)ã€‚â€¨âœ” è¿™å°±æ˜¯ â€œW2 æŒ‰è¡Œåˆ‡â€ã€‚
####æ‰€ä»¥å’Œä½ è¯´çš„ çœŸå®žçŽ¯å¢ƒå¸¸è§åšæ³•æ˜¯ä¸€è‡´çš„ã€‚
####
####è¦ä¸è¦æˆ‘å¸®ä½ åœ¨ä»£ç é‡ŒåŠ ä¸€æ®µ å›¾ç¤ºï¼ˆçŸ©é˜µåˆ‡åˆ†å¯è§†åŒ–ï¼‰ï¼Œè¿™æ ·å¯ä»¥æ¸…æ¥šçœ‹åˆ°ï¼š
####W1 å¦‚ä½•æŒ‰åˆ—åˆ‡åˆ†
####W2 å¦‚ä½•æŒ‰è¡Œåˆ‡åˆ†â€¨ä»¥åŠ forward/backward ä¸­å„ä¸ª shard çš„æ•°æ®æµï¼Ÿ
####
####
####
####å¥½ ðŸ‘Œ æˆ‘æ¥å¸®ä½ æŠŠ Tensor Parallel çš„çŸ©é˜µåˆ‡åˆ†æ–¹å¼ ç”¨å›¾ç¤ºç›´è§‚å±•ç¤ºä¸€ä¸‹ã€‚
####
####ðŸ”¹ 1. W1 æŒ‰åˆ—åˆ‡åˆ†ï¼ˆColumn Parallelï¼‰
####è®¾è¾“å…¥å±‚ç»´åº¦æ˜¯ D_inï¼Œéšå±‚ç»´åº¦æ˜¯ D_hiddenã€‚â€¨æƒé‡çŸ©é˜µ W1 çš„å½¢çŠ¶æ˜¯ (D_in, D_hidden)ã€‚
####æ­£å¸¸æƒ…å†µï¼š
####
####X (N, D_in)  @  W1 (D_in, D_hidden)  â†’  H (N, D_hidden)
####åˆ—åˆ‡åˆ†ï¼š
####
####W1 è¢«æŒ‰åˆ—åˆ‡åˆ† â†’ æ¯ä¸ª rank æ‹¿ä¸€éƒ¨åˆ† hidden
####ä¾‹å­ï¼šâ€¨å‡è®¾ D_in = 4, D_hidden = 6, åˆ‡æˆ 2 ä»½ï¼š
####
####         W1 (4x6)
####   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
####   â”‚      Rank0    â”‚      Rank1
####   â”‚   (4x3 cols)  â”‚   (4x3 cols)
####   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
####æ¯ä¸ª rank ç®—ï¼š
####
####H0 = X @ W1_shard0   â†’ (N, 3)
####H1 = X @ W1_shard1   â†’ (N, 3)
####æœ€åŽæ‹¼æŽ¥ï¼š
####
####H = [H0 | H1]  â†’ (N, 6)
####
####ðŸ”¹ 2. W2 æŒ‰è¡Œåˆ‡åˆ†ï¼ˆRow Parallelï¼‰
####è®¾éšå±‚ç»´åº¦æ˜¯ D_hiddenï¼Œè¾“å‡ºå±‚ç»´åº¦æ˜¯ D_outã€‚â€¨æƒé‡çŸ©é˜µ W2 çš„å½¢çŠ¶æ˜¯ (D_hidden, D_out)ã€‚
####æ­£å¸¸æƒ…å†µï¼š
####
####H (N, D_hidden)  @  W2 (D_hidden, D_out)  â†’  Y (N, D_out)
####è¡Œåˆ‡åˆ†ï¼š
####
####W2 è¢«æŒ‰è¡Œåˆ‡åˆ† â†’ æ¯ä¸ª rank æ‹¿ä¸€éƒ¨åˆ†è¾“å…¥ hidden
####ä¾‹å­ï¼šâ€¨å‡è®¾ D_hidden = 6, D_out = 2, åˆ‡æˆ 2 ä»½ï¼š
####
####         W2 (6x2)
####   â”Œâ”€â”€â”€â”€â”€â”€â”€â”
####   â”‚ Rank0 â”‚  (3x2 rows)
####   â”‚ Rank1 â”‚  (3x2 rows)
####   â””â”€â”€â”€â”€â”€â”€â”€â”˜
####æ¯ä¸ª rank ç®—ï¼š
####
####Y0 = H[:, :3] @ W2_shard0   â†’ (N, 2)
####Y1 = H[:, 3:] @ W2_shard1   â†’ (N, 2)
####æœ€åŽ åŠ å’Œ (AllReduce)ï¼š
####
####Y = Y0 + Y1   â†’ (N, 2)
####
####ðŸ”¹ 3. æ€»ç»“æ•°æ®æµ
####
####X â”€â”€â–º W1 (col split) â”€â”€â–º æ‹¼æŽ¥ H â”€â”€â–º W2 (row split) â”€â”€â–º AllReduce â”€â”€â–º Y
####W1ï¼šåˆ—åˆ‡ â†’ è¾“å‡ºæ‹¼æŽ¥ï¼ˆConcatï¼‰
####W2ï¼šè¡Œåˆ‡ â†’ è¾“å‡ºæ±‚å’Œï¼ˆReduceï¼‰
####
####è¦ä¸è¦æˆ‘ç›´æŽ¥æŠŠè¿™ä¸ª å¯è§†åŒ–å›¾è§£ï¼ˆç”¨ matplotlib æˆ– ascii blockï¼‰é›†æˆåˆ°æˆ‘ä¹‹å‰å†™çš„æ•™å­¦ä»£ç é‡Œï¼Œè¿™æ ·ä½ è·‘çš„æ—¶å€™å°±èƒ½çœ‹åˆ°çŸ©é˜µæ€Žä¹ˆåˆ‡åˆ†ã€æ•°æ®æ€Žä¹ˆæµåŠ¨ï¼Ÿ
####
####
####
####è¯·åŠ åå‘ä¼ æ’­çš„ ascii blockï¼Œæ˜¾ç¤º grad_W1 å’Œ grad_W2 æ˜¯æ€Žä¹ˆåˆå¹¶çš„
####
####
####
####å¥½ âœ…â€¨åœ¨å‰å‘æˆ‘ä»¬å·²ç»ç”»äº† W1 åˆ—åˆ‡ / W2 è¡Œåˆ‡ï¼ŒçŽ°åœ¨æˆ‘æ¥å¸®ä½ è¡¥ä¸Š åå‘ä¼ æ’­çš„ ascii blockï¼Œå¹¶ä¸”æŠŠå®ƒé›†æˆåˆ°ä»£ç é‡Œã€‚è¿™æ ·ä¸€è·‘ï¼Œä¸å…‰èƒ½çœ‹åˆ° forward çš„æ•°æ®æµï¼Œè¿˜èƒ½çœ‹åˆ° backward çš„æ¢¯åº¦æµã€‚
####
####ðŸ”¹ åå‘ä¼ æ’­ç›´è§‚é€»è¾‘
####Loss å¯¹è¾“å‡ºçš„æ¢¯åº¦
####
####dL/dY  (N, D_out)
####W2ï¼ˆè¡Œåˆ‡åˆ†ï¼‰çš„åä¼ 
####forward æ—¶ W2 æ˜¯ è¡Œåˆ‡ â†’ æ¯ä¸ª rank è´Ÿè´£ä¸€éƒ¨åˆ† hidden â†’ åä¼ æ—¶æ¯ä¸ª rank åªæ”¶åˆ°å¯¹åº” shard çš„ grad_H_localã€‚
####
####grad_W2_rank = H_localáµ€ @ dL/dY
####grad_H_local = dL/dY @ W2_shardáµ€
####æœ€åŽï¼š
####
####grad_W2 = sum(grad_W2_rank)   (è¡Œæ‹¼èµ·æ¥)
####grad_H  = concat(grad_H_local)  (åˆ—æ‹¼èµ·æ¥)
####W1ï¼ˆåˆ—åˆ‡åˆ†ï¼‰çš„åä¼ 
####forward æ—¶ W1 æ˜¯ åˆ—åˆ‡ â†’ æ¯ä¸ª rank åªè®¡ç®—éƒ¨åˆ† H â†’ backward æ—¶è¦ sum å„ä¸ª shard å¯¹è¾“å…¥çš„æ¢¯åº¦è´¡çŒ®ï¼š
####
####grad_W1_rank = Xáµ€ @ grad_H_local
####grad_X       = sum_over_ranks(grad_H_local @ W1_shardáµ€)
####æœ€åŽï¼š
####
####grad_W1 = sum(grad_W1_rank)


import numpy as np
from multiprocessing import Process, Queue
import time

# Hyperparameters
input_size = 784  # e.g., flattened MNIST images
hidden_size = 256
output_size = 10  # 10 classes
learning_rate = 0.001  # For Adam
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8
batch_size = 64
epochs = 2  # Reduced for demo
num_devices = 2  # For parallel implementations


# Activation functions
def relu(x):
    return np.maximum(0, x)


def relu_deriv(x):
    return (x > 0).astype(float)


def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)


# Cross-entropy loss
def cross_entropy_loss(y_pred, y_true):
    m = y_pred.shape[0]
    log_likelihood = -np.log(y_pred[np.arange(m), y_true.argmax(axis=1)] + 1e-15)
    return np.mean(log_likelihood)


# Adam optimizer parameters initialization for full params
def initialize_adam_params(W1, b1, W2, b2):
    return {
        'm_W1': np.zeros_like(W1),
        'v_W1': np.zeros_like(W1),
        'm_b1': np.zeros_like(b1),
        'v_b1': np.zeros_like(b1),
        'm_W2': np.zeros_like(W2),
        'v_W2': np.zeros_like(W2),
        'm_b2': np.zeros_like(b2),
        'v_b2': np.zeros_like(b2)
    }


# Adam update for full params
def update_with_adam(param, dparam, m, v, t, key):
    m = beta1 * m + (1 - beta1) * dparam
    v = beta2 * v + (1 - beta2) * (dparam ** 2)
    m_hat = m / (1 - beta1 ** t)
    v_hat = v / (1 - beta2 ** t)
    param -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)
    return m, v


# Single-device MLP training with Adam
def train_single_device(X, Y_one_hot):
    print("\n=== Single Device Training with Adam ===")
    W1 = np.random.randn(input_size, hidden_size) * 0.01
    b1 = np.zeros(hidden_size)
    W2 = np.random.randn(hidden_size, output_size) * 0.01
    b2 = np.zeros(output_size)
    adam_params = initialize_adam_params(W1, b1, W2, b2)
    t = 1

    for epoch in range(epochs):
        indices = np.random.permutation(X.shape[0])
        X_shuffled = X[indices]
        Y_shuffled = Y_one_hot[indices]
        running_loss = 0.0
        num_batches = X.shape[0] // batch_size

        for i in range(0, X.shape[0], batch_size):
            X_batch = X_shuffled[i:i + batch_size]
            Y_batch = Y_shuffled[i:i + batch_size]

            # Forward pass
            Z1 = X_batch @ W1 + b1
            A1 = relu(Z1)
            Z2 = A1 @ W2 + b2
            A2 = softmax(Z2)

            # Compute loss
            loss = cross_entropy_loss(A2, Y_batch)
            running_loss += loss

            # Backward pass
            m_batch = X_batch.shape[0]
            dZ2 = A2 - Y_batch
            dW2 = (A1.T @ dZ2) / m_batch
            db2 = np.sum(dZ2, axis=0) / m_batch
            dA1 = dZ2 @ W2.T
            dZ1 = dA1 * relu_deriv(Z1)
            dW1 = (X_batch.T @ dZ1) / m_batch
            db1 = np.sum(dZ1, axis=0) / m_batch

            # Update with Adam
            adam_params['m_W1'], adam_params['v_W1'] = update_with_adam(W1, dW1, adam_params['m_W1'],
                                                                        adam_params['v_W1'], t, 'W1')
            adam_params['m_b1'], adam_params['v_b1'] = update_with_adam(b1, db1, adam_params['m_b1'],
                                                                        adam_params['v_b1'], t, 'b1')
            adam_params['m_W2'], adam_params['v_W2'] = update_with_adam(W2, dW2, adam_params['m_W2'],
                                                                        adam_params['v_W2'], t, 'W2')
            adam_params['m_b2'], adam_params['v_b2'] = update_with_adam(b2, db2, adam_params['m_b2'],
                                                                        adam_params['v_b2'], t, 'b2')
            t += 1

        avg_loss = running_loss / num_batches
        print(f"[Single] Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}")

    return W1, b1, W2, b2


# Data parallel training with Adam
def data_parallel_worker(X_batch, Y_batch, W1, b1, W2, b2, queue):
    # Forward pass
    Z1 = X_batch @ W1 + b1
    A1 = relu(Z1)
    Z2 = A1 @ W2 + b2
    A2 = softmax(Z2)

    # Compute loss
    m_batch = X_batch.shape[0]
    loss = cross_entropy_loss(A2, Y_batch)

    # Backward pass
    dZ2 = A2 - Y_batch
    dW2 = (A1.T @ dZ2) / m_batch
    db2 = np.sum(dZ2, axis=0) / m_batch
    dA1 = dZ2 @ W2.T
    dZ1 = dA1 * relu_deriv(Z1)
    dW1 = (X_batch.T @ dZ1) / m_batch
    db1 = np.sum(dZ1, axis=0) / m_batch

    queue.put((dW1, db1, dW2, db2, loss))


def train_data_parallel(X, Y_one_hot, num_devices=2):
    print("\n=== Data Parallel Training with Adam ===")
    W1 = np.random.randn(input_size, hidden_size) * 0.01
    b1 = np.zeros(hidden_size)
    W2 = np.random.randn(hidden_size, output_size) * 0.01
    b2 = np.zeros(output_size)
    adam_params = initialize_adam_params(W1, b1, W2, b2)
    t = 1

    for epoch in range(epochs):
        indices = np.random.permutation(X.shape[0])
        X_shuffled = X[indices]
        Y_shuffled = Y_one_hot[indices]
        running_loss = 0.0
        num_batches = X.shape[0] // batch_size

        for i in range(0, X.shape[0], batch_size):
            X_batch = X_shuffled[i:i + batch_size]
            Y_batch = Y_shuffled[i:i + batch_size]

            # Split batch across devices
            batch_indices = np.array_split(range(X_batch.shape[0]), num_devices)
            processes = []
            queues = [Queue() for _ in range(num_devices)]
            for d in range(num_devices):
                p = Process(target=data_parallel_worker,
                            args=(X_batch[batch_indices[d]], Y_batch[batch_indices[d]], W1, b1, W2, b2, queues[d]))
                processes.append(p)
                p.start()

            # Collect gradients
            dW1_sum = np.zeros_like(W1)
            db1_sum = np.zeros_like(b1)
            dW2_sum = np.zeros_like(W2)
            db2_sum = np.zeros_like(b2)
            batch_loss = 0.0
            for q in queues:
                dW1, db1, dW2, db2, loss = q.get()
                dW1_sum += dW1
                db1_sum += db1
                dW2_sum += dW2
                db2_sum += db2
                batch_loss += loss
            dW1 = dW1_sum / num_devices
            db1 = db1_sum / num_devices
            dW2 = dW2_sum / num_devices
            db2 = db2_sum / num_devices
            running_loss += batch_loss / num_devices

            # Update with Adam
            adam_params['m_W1'], adam_params['v_W1'] = update_with_adam(W1, dW1, adam_params['m_W1'],
                                                                        adam_params['v_W1'], t, 'W1')
            adam_params['m_b1'], adam_params['v_b1'] = update_with_adam(b1, db1, adam_params['m_b1'],
                                                                        adam_params['v_b1'], t, 'b1')
            adam_params['m_W2'], adam_params['v_W2'] = update_with_adam(W2, dW2, adam_params['m_W2'],
                                                                        adam_params['v_W2'], t, 'W2')
            adam_params['m_b2'], adam_params['v_b2'] = update_with_adam(b2, db2, adam_params['m_b2'],
                                                                        adam_params['v_b2'], t, 'b2')
            t += 1

            for p in processes:
                p.join()

        avg_loss = running_loss / num_batches
        print(f"[Data Parallel] Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}")

    return W1, b1, W2, b2


# Tensor parallel worker
def tensor_parallel_worker(device_id, num_devices, hidden_split, X_batch, Y_batch, W1_part, b1_part, W2_part, b2,
                           queue_in, queue_out):
    m_batch = X_batch.shape[0]

    # Forward first layer: column-parallel
    Z1_part = X_batch @ W1_part + b1_part  # Shape: (batch_size, hidden_split), e.g., (64, 128)
    A1_part = relu(Z1_part)

    # Forward second layer: row-parallel
    Z2_local = A1_part @ W2_part + b2 / num_devices  # Shape: (batch_size, output_size), e.g., (64, 10)

    # Send Z2_local for all-reduce (sum)
    queue_out.put(Z2_local)

    # Receive full Z2
    Z2 = queue_in.get()

    # Compute A2, dZ2, partial loss
    A2 = softmax(Z2)
    dZ2 = A2 - Y_batch
    partial_loss = cross_entropy_loss(A2, Y_batch)

    # Backward second layer: row-parallel
    dA1_local = dZ2 @ W2_part.T  # Shape: (batch_size, hidden_split), e.g., (64, 128)
    dW2_part = (A1_part.T @ dZ2) / m_batch  # Shape: (hidden_split, output_size), e.g., (128, 10)
    # db2 computed in main

    # Send dA1_local for all-gather (concat)
    queue_out.put(dA1_local)

    # Receive full dA1
    dA1 = queue_in.get()

    # Backward first layer: column-parallel
    dA1_slice = dA1[:, device_id * hidden_split: (device_id + 1) * hidden_split]  # Shape: (batch_size, hidden_split)
    dZ1_part = dA1_slice * relu_deriv(Z1_part)
    dW1_part = (X_batch.T @ dZ1_part) / m_batch  # Shape: (input_size, hidden_split), e.g., (784, 128)
    db1_part = np.sum(dZ1_part, axis=0) / m_batch  # Shape: (hidden_split,), e.g., (128,)

    # Send grads and partial loss
    queue_out.put((dW1_part, db1_part, dW2_part, partial_loss))


def train_tensor_parallel(X, Y_one_hot, num_devices=2):
    print("\n=== Tensor Parallel Training with Adam ===")
    # Print visualizations
    print("\nTP Visualization - W1 Split (Column-Wise):")
    print("""
  X (64 x 784)
    |
    v
+---------+
| W1      |
| 784 x 256|
+---------+
Split column-wise:
+---------+---------+
| Device 0| Device 1|
| 784 x 128| 784 x 128|
+---------+---------+
    |         |
    v         v
 A1_part0  A1_part1 (64 x 128)
 (64 x 128)
 (No All-Gather for A1; used locally for Z2_local)
""")

    print("\nTP Visualization - W2 Split (Row-Wise):")
    print("""
 A1_part (64 x 128)
    |
    v
+---------+
| W2      |
| 256 x 10|
+---------+
Split row-wise:
+---------+
| Device 0|
| 128 x 10|
+---------+
+---------+
| Device 1|
| 128 x 10|
+---------+
    |       
    v       
 Z2_local0 (64 x 10)
    |
    v
 Z2_local1 (64 x 10)
    |
    +--------+
      All-Reduce (sum)
        |
        v
 Z2 (64 x 10)
""")

    print("\nTP Visualization - Backward Flow:")
    print("""
 dZ2 (64 x 10)
    |
    v
dA1_local = dZ2 @ W2_part.T (64 x 128)
    |
    +--------+
      All-Gather (concat)
        |
        v
 dA1 (64 x 256)
Split to dA1_slice (64 x 128)
    |
    v
 dZ1_part = dA1_slice * relu_deriv (64 x 128)
    |
    v
 dW1_part = X.T @ dZ1_part / m (784 x 128)
 dW2_part = A1_part.T @ dZ2 / m (128 x 10)
""")

    W1 = np.random.randn(input_size, hidden_size) * 0.01
    b1 = np.zeros(hidden_size)
    W2 = np.random.randn(hidden_size, output_size) * 0.01
    b2 = np.zeros(output_size)
    adam_params = initialize_adam_params(W1, b1, W2, b2)
    t = 1

    hidden_split = hidden_size // num_devices  # e.g., 256 / 2 = 128

    # W1: column-wise split, W2: row-wise split
    W1_parts = [W1[:, i * hidden_split:(i + 1) * hidden_split] for i in range(num_devices)]  # Shape: (784, 128)
    b1_parts = [b1[i * hidden_split:(i + 1) * hidden_split] for i in range(num_devices)]  # Shape: (128,)
    W2_parts = [W2[i * hidden_split:(i + 1) * hidden_split, :] for i in range(num_devices)]  # Shape: (128, 10)
    b2 = b2  # Replicated across devices

    for epoch in range(epochs):
        indices = np.random.permutation(X.shape[0])
        X_shuffled = X[indices]
        Y_shuffled = Y_one_hot[indices]
        running_loss = 0.0
        num_batches = X.shape[0] // batch_size

        for i in range(0, X.shape[0], batch_size):
            X_batch = X_shuffled[i:i + batch_size]
            Y_batch = Y_shuffled[i:i + batch_size]

            # Start processes
            processes = []
            queue_ins = [Queue() for _ in range(num_devices)]
            queue_outs = [Queue() for _ in range(num_devices)]
            for d in range(num_devices):
                p = Process(target=tensor_parallel_worker, args=(
                d, num_devices, hidden_split, X_batch, Y_batch, W1_parts[d], b1_parts[d], W2_parts[d], b2, queue_ins[d],
                queue_outs[d]))
                processes.append(p)
                p.start()

            # Collect Z2_local, sum to Z2 (all-reduce)
            Z2_locals = [queue_outs[d].get() for d in range(num_devices)]
            Z2 = np.sum(Z2_locals, axis=0)  # Shape: (batch_size, output_size), e.g., (64, 10)

            # Send Z2 to all devices
            for q in queue_ins:
                q.put(Z2)

            # Collect dA1_local, concatenate to dA1 (all-gather)
            dA1_locals = [queue_outs[d].get() for d in range(num_devices)]
            dA1 = np.concatenate(dA1_locals, axis=1)  # Shape: (batch_size, hidden_size), e.g., (64, 256)

            # Send dA1 to all devices
            for q in queue_ins:
                q.put(dA1)

            # Collect gradients and partial loss
            dW1_parts_new = [None] * num_devices
            db1_parts_new = [None] * num_devices
            dW2_parts_new = [None] * num_devices
            partial_losses = [None] * num_devices
            for d in range(num_devices):
                dW1_part, db1_part, dW2_part, partial_loss = queue_outs[d].get()
                dW1_parts_new[d] = dW1_part
                db1_parts_new[d] = db1_part
                dW2_parts_new[d] = dW2_part
                partial_losses[d] = partial_loss

            # Aggregate gradients
            dW1 = np.concatenate(dW1_parts_new, axis=1)  # Shape: (input_size, hidden_size), e.g., (784, 256)
            db1 = np.concatenate(db1_parts_new)  # Shape: (hidden_size,), e.g., (256,)
            dW2 = np.concatenate(dW2_parts_new, axis=0)  # Shape: (hidden_size, output_size), e.g., (256, 10)

            # Compute db2 in main
            A2 = softmax(Z2)
            dZ2 = A2 - Y_batch
            db2 = np.sum(dZ2, axis=0) / batch_size  # Shape: (output_size,), e.g., (10,)

            running_loss += sum(partial_losses) / num_devices

            # Update with Adam
            adam_params['m_W1'], adam_params['v_W1'] = update_with_adam(W1, dW1, adam_params['m_W1'],
                                                                        adam_params['v_W1'], t, 'W1')
            adam_params['m_b1'], adam_params['v_b1'] = update_with_adam(b1, db1, adam_params['m_b1'],
                                                                        adam_params['v_b1'], t, 'b1')
            adam_params['m_W2'], adam_params['v_W2'] = update_with_adam(W2, dW2, adam_params['m_W2'],
                                                                        adam_params['v_W2'], t, 'W2')
            adam_params['m_b2'], adam_params['v_b2'] = update_with_adam(b2, db2, adam_params['m_b2'],
                                                                        adam_params['v_b2'], t, 'b2')
            t += 1

            # Update weight parts
            W1_parts = [W1[:, i * hidden_split:(i + 1) * hidden_split] for i in range(num_devices)]
            b1_parts = [b1[i * hidden_split:(i + 1) * hidden_split] for i in range(num_devices)]
            W2_parts = [W2[i * hidden_split:(i + 1) * hidden_split, :] for i in range(num_devices)]

            for p in processes:
                p.join()

        avg_loss = running_loss / num_batches
        print(f"[Tensor Parallel] Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}")

    return W1, b1, W2, b2


# Evaluate accuracy
def evaluate(X, Y, W1, b1, W2, b2):
    Z1 = X @ W1 + b1
    A1 = relu(Z1)
    Z2 = A1 @ W2 + b2
    A2 = softmax(Z2)
    predictions = np.argmax(A2, axis=1)
    return np.mean(predictions == Y)


if __name__ == '__main__':
    # Generate synthetic dataset
    np.random.seed(42)
    X = np.random.randn(1000, input_size)  # 1000 samples x 784 features
    Y = np.random.randint(0, output_size, 1000)  # Integer labels (0-9)
    Y_one_hot = np.zeros((1000, output_size))
    Y_one_hot[np.arange(1000), Y] = 1  # One-hot encoded labels

    # Run training
    print("Training with Single Device")
    #W1_s, b1_s, W2_s, b2_s = train_single_device(X, Y_one_hot)

    print("\nTraining with Data Parallel")
    #W1_dp, b1_dp, W2_dp, b2_dp = train_data_parallel(X, Y_one_hot)

    print("\nTraining with Tensor Parallel")
    W1_tp, b1_tp, W2_tp, b2_tp = train_tensor_parallel(X, Y_one_hot)

    # Evaluate accuracies
    #print(f"\nSingle Device Accuracy: {evaluate(X, Y, W1_s, b1_s, W2_s, b2_s):.4f}")
    #print(f"Data Parallel Accuracy: {evaluate(X, Y, W1_dp, b1_dp, W2_dp, b2_dp):.4f}")
    print(f"Tensor Parallel Accuracy: {evaluate(X, Y, W1_tp, b1_tp, W2_tp, b2_tp):.4f}")